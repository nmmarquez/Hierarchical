---
title: "Homework 7"
author: "Neal Marquez"
date: "March 1, 2019"
output:
  pdf_document:
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    fig.width=4.8, fig.height=3.6, echo=FALSE, warning=FALSE, message=FALSE)
```

## Question 1  
_Data on the amount of time students from two high schools spent on studying/homework(in hours) during an exam period are as follows:_  


School 1:  
2.1 9.8 13.9 11.3 8.9 15.7 16.4 4.5 8.9 11.9 12.5 11.1 11.6 14.5 9.6 7.4 3.3 9.1 9.4 6.0 7.4 8.5 1.6 11.4 9.7  

School 2:  
0.3 1.1 6.5 11.7 6.5 5.6 14.6 11.7 9.1 9.4 10.6 12.3 9.5 0.6 15.4 5.3 8.5 3.0 3.8 6.2 2.1 6.6 1.1  

_Carry out a Bayesian analysis of the data from each of these schools separately, using the normal model with a conjugate prior distribution, in which $\mu_0$= 5, $\sigma_0^2$= 4, $\kappa_0$= 1, and $\nu_0$= 2, in the notation used in class:_  

### A) Compute or approximate posterior means and 95% confidence regions for the mean and standard deviation from each school.  

$$
\begin{aligned}
p(\theta, \sigma^2 | y_1, \dots, y_n) &\propto
    p(y_1, \dots, y_n | \theta, \sigma^2) p(\theta, \sigma^2) \\
p(\theta, \sigma^2) &= p(\theta|\sigma^2) p(\sigma^2) \\
\end{aligned}
$$

#### Priors  
$$
\begin{aligned}
\theta | \sigma^2 &\sim \mathcal{N}(\mu_0, \frac{\sigma^2}{\kappa_0}) \\
\frac{1}{\sigma^2} &\sim \text{Gamma}(\frac{\nu_0}{2},\frac{\nu_0}{2} \sigma_0^2) \\
\end{aligned}
$$

#### Posterior    
$$
\begin{aligned}
\theta | \sigma^2, y_1, \dots, y_n &\sim \mathcal{N}\Bigg(
    \frac{\kappa_0 \mu_0 + n \overline y}{\kappa_0 + n} , 
    \frac{\sigma^2}{\kappa_0 + n} \Bigg) \\
\frac{1}{\sigma^2} | y_1, \dots, y_n &\sim \text{Gamma} \Bigg( 
    \frac{\nu_0 + n}{2},
    .5 \Big(\nu_0 \sigma^2_0 + (n-1)s^2 + \frac{\kappa_0 n}{\kappa_0 + n}
    (\overline y -\mu_0)^2 \Big)\Bigg)
\end{aligned}
$$

```{r}
library(tidyverse)
set.seed(123)
N <- 10000

# data
y1 <- c(2.1, 9.8, 13.9, 11.3, 8.9, 15.7, 16.4, 4.5, 8.9, 11.9, 12.5, 11.1, 11.6,
      14.5, 9.6, 7.4, 3.3, 9.1, 9.4, 6.0, 7.4, 8.5, 1.6, 11.4, 9.7)

y2 <- c(0.3, 1.1, 6.5, 11.7, 6.5, 5.6, 14.6, 11.7, 9.1, 9.4, 10.6, 12.3, 9.5, 
        0.6, 15.4, 5.3, 8.5, 3.0, 3.8, 6.2, 2.1, 6.6, 1.1)

mcmcNormalPosterior <- function(y, N=10000, mu0 = 5, k0 = 1, s20 = 4, nu0 = 2){
    n <- length(y)
    ybar <- mean(y)
    s2 <- var ( y )
    # posterior inference
    kn <- k0+n 
    nun <- nu0+n
    mun <- ( k0 *mu0 + n* ybar ) / kn
    s2n <- (nu0 * s20 +(n-1)* s2 +k0 *n * ( ybar-mu0 )^2 / (kn))/(nun)
    
    sigma2Posterior <- rgamma(N, nun/2, nun *s2n*.5)^-1
    muPosterior <- rnorm(N, mun, sqrt(sigma2Posterior / kn))
    
    tibble(postMu=muPosterior, postSig2=sigma2Posterior)
}

postQ1 <- bind_rows(
    mcmcNormalPosterior(y1) %>%
        mutate(school=1),
    mcmcNormalPosterior(y2) %>%
        mutate(school=2)) 

postQ1 %>%
    mutate(postSig=sqrt(postSig2)) %>%
    select(-postSig2) %>%
    group_by(school) %>%
    mutate(medMu=median(postMu), medSig=median(postSig)) %>%
    ungroup() %>%
    ggplot(aes(x=postMu, y=postSig)) +
    geom_density_2d() +
    facet_wrap(~school) +
    theme_classic() +
    geom_hline(aes(yintercept = medSig), linetype=3, color="red") +
    geom_vline(aes(xintercept = medMu), linetype=3, color="red")
```